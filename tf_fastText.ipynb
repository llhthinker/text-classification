{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  177\n",
      "max sentence length:  137\n",
      "[['Wall', 'St.', 'Bears', 'Claw', 'Back', 'Into', 'the', 'Black', '(Reuters)', 'Reuters', '-', 'Short-sellers,', 'Wall', \"Street's\", 'dwindling\\\\band', 'of', 'ultra-cynics,', 'are', 'seeing', 'green', 'again.'], ['Carlyle', 'Looks', 'Toward', 'Commercial', 'Aerospace', '(Reuters)', 'Reuters', '-', 'Private', 'investment', 'firm', 'Carlyle', 'Group,\\\\which', 'has', 'a', 'reputation', 'for', 'making', 'well-timed', 'and', 'occasionally\\\\controversial', 'plays', 'in', 'the', 'defense', 'industry,', 'has', 'quietly', 'placed\\\\its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market.'], ['Oil', 'and', 'Economy', 'Cloud', \"Stocks'\", 'Outlook', '(Reuters)', 'Reuters', '-', 'Soaring', 'crude', 'prices', 'plus', 'worries\\\\about', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'to\\\\hang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'the\\\\summer', 'doldrums.']]\n",
      "[3, 3, 3]\n",
      "Length of train data:  120000\n",
      "Length of test data:  7600\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    载入原始数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    csv_reader = csv.reader(open(data_path, encoding='utf-8'))\n",
    "    max_sentence_len = 0\n",
    "    for line in csv_reader:\n",
    "        one_data = line[1].split()\n",
    "        one_data.extend(line[2].split())\n",
    "        if len(one_data) > max_sentence_len:\n",
    "            max_sentence_len = len(one_data)\n",
    "        data.append(one_data)\n",
    "        labels.append(int(line[0]))\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels\n",
    "\n",
    "train_data, train_labels = load_data(\"./data/ag_news_csv/train.csv\")\n",
    "test_data, test_labels = load_data(\"./data/ag_news_csv/test.csv\")\n",
    "print(train_data[:3])\n",
    "print(train_labels[:3])\n",
    "print(\"Length of train data: \", len(train_data))\n",
    "print(\"Length of test data: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 10 common words:  [['UNK', -1], ('the', 188763), ('to', 125072), ('a', 104353), ('of', 103519), ('in', 97687), ('and', 72345), ('on', 58606), ('for', 51494), ('-', 41545)]\n"
     ]
    }
   ],
   "source": [
    "def build_voabulary(train_data, test_data, vocabulary_size=50000):\n",
    "    \"\"\"\n",
    "    基于所有数据构建词表\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    words = []\n",
    "    for line in train_data:\n",
    "        words.extend(line)\n",
    "    for line in test_data:\n",
    "        words.extend(line)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dict_word2index = dict()\n",
    "    for word, _ in count:\n",
    "        dict_word2index[word] = len(dict_word2index)\n",
    "    dict_index2word = dict(zip(dict_word2index.values(), dict_word2index.keys()))\n",
    "    \n",
    "    return  count, dict_word2index, dict_index2word\n",
    "\n",
    "count, dict_word2index, dict_index2word = build_voabulary(train_data, test_data, vocabulary_size=100000)\n",
    "print(\"Most 10 common words: \", count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 200)\n",
      "(120000, 4)\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(data, labels, dict_word2index, max_sentence_len=200, label_size=4):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        one_label = [0] * label_size\n",
    "        one_label[labels[i]-1] = 1\n",
    "        new_labels.append(one_label) \n",
    "        new_line = []\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                index = dict_word2index[word]\n",
    "            else:\n",
    "                index = 0    # UNK\n",
    "            new_line.append(index)\n",
    "        \n",
    "        zero_num = max_sentence_len - len(new_line)\n",
    "        while zero_num > 0:\n",
    "            new_line.append(0)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_line)\n",
    "    return np.array(dataset, dtype=np.int32), np.array(new_labels, dtype=np.int32)\n",
    "\n",
    "train_dataset, train_labels = build_dataset(train_data, train_labels, dict_word2index)\n",
    "test_dataset, test_labels = build_dataset(test_data, test_labels, dict_word2index)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84000, 200)\n",
      "(84000, 4)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, radio=0.7):\n",
    "    \"\"\"\n",
    "    将训练集分给为训练集和检验集\n",
    "    \"\"\"\n",
    "    split_index = int(len(data) * 0.7)\n",
    "    new_data1 = data[ : split_index]\n",
    "    new_data2 = data[split_index : ]\n",
    "    return new_data1, new_data2\n",
    "\n",
    "train_X, valid_X = split_data(train_dataset)\n",
    "train_y, valid_y = split_data(train_labels)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面开始构建计算图并进行训练，通过实验发现：\n",
    "- 将num_steps从1001提高到2001，对模型效果有显著提升\n",
    "- 在该数据集下，将vocabulary_size分别设置为50000和100000对检验集上的accuracy影响不大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "# num_sampled = 4 # Number of negative examples to sample.\n",
    "max_sentence_len = 200\n",
    "vocabulary_size = 100000\n",
    "label_size = 4\n",
    "graph = tf.Graph()\n",
    "    \n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.int32, shape=[batch_size, max_sentence_len])\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=[batch_size,label_size])\n",
    "  tf_valid_dataset = tf.constant(valid_X, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([label_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([label_size]))\n",
    "  \n",
    "  def model(data): \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    # embedding_lookup()用法: http://blog.csdn.net/u013041398/article/details/60955847\n",
    "    embed = tf.nn.embedding_lookup(embeddings, data)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    sentence_embed = tf.reduce_mean(embed, axis=1)\n",
    "    \n",
    "    return tf.matmul(sentence_embed, tf.transpose(softmax_weights)) + softmax_biases\n",
    "\n",
    "\n",
    "  logits = model(tf_train_dataset)\n",
    "    \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \"\"\"\n",
    "  由于label_size=4，比较小，不需要进行Hierarchical softmax\n",
    "  loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                tf.nn.nce_loss(weights=softmax_weights,  #[embed_size, label_size]--->[label_size,embed_size]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n",
    "                               biases=softmax_biases,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n",
    "                               labels=train_labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n",
    "                               inputs=sentence_embed,# [None,self.embed_size] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                               num_sampled=num_sampled,  #scalar. 100\n",
    "                               num_classes=label_size,partition_strategy=\"div\"))\n",
    "  \"\"\"   \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  learning_rate = tf.train.exponential_decay(0.01, global_step, 1000, 0.9, staircase=True)\n",
    "  optimizer = tf.contrib.layers.optimize_loss(loss, global_step=global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#   test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.41933 train accuracy: 25.0\n",
      "loss: 1.34674 train accuracy: 37.5\n",
      "loss: 1.08823 train accuracy: 62.5\n",
      "loss: 0.557154 train accuracy: 75.0\n",
      "loss: 0.476505 train accuracy: 87.5\n",
      "loss: 0.476347 train accuracy: 75.0\n",
      "loss: 0.410188 train accuracy: 87.5\n",
      "loss: 0.553505 train accuracy: 75.0\n",
      "loss: 0.3854 train accuracy: 93.75\n",
      "loss: 0.210102 train accuracy: 93.75\n",
      "loss: 0.105567 train accuracy: 100.0\n",
      "loss: 0.338261 train accuracy: 81.25\n",
      "loss: 0.393003 train accuracy: 81.25\n",
      "loss: 0.253228 train accuracy: 93.75\n",
      "loss: 0.26827 train accuracy: 87.5\n",
      "loss: 0.544425 train accuracy: 93.75\n",
      "valid accuracy: 88.6194444444\n",
      "loss: 0.185084 train accuracy: 93.75\n",
      "valid accuracy: 87.0861111111\n",
      "loss: 0.15021 train accuracy: 93.75\n",
      "valid accuracy: 90.1055555556\n",
      "loss: 0.35806 train accuracy: 93.75\n",
      "valid accuracy: 88.3388888889\n",
      "loss: 0.51087 train accuracy: 81.25\n",
      "valid accuracy: 90.2694444444\n",
      "loss: 0.266149 train accuracy: 87.5\n",
      "valid accuracy: 90.5388888889\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_y.shape[0] - batch_size)\n",
    "    batch_data = train_X[offset:(offset + batch_size)]\n",
    "    batch_labels = train_y[offset:(offset + batch_size)]\n",
    "\n",
    "    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "    _, l,tp = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print(\"loss:\",l,\"acc:\",a,\"label:\",batch_labels,\"prediction:\",p)\n",
    "    if (step % 100 == 0):\n",
    "        train_acc = accuracy(tp, batch_labels)\n",
    "        print(\"loss:\", l, \"train accuracy:\", train_acc)\n",
    "        if step > num_steps * 0.7:\n",
    "            print( \"valid accuracy:\",accuracy(valid_prediction.eval(), valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面尝试使用L2 loss对模型进行regularization\n",
    "- 当同时惩罚参数embeddings和softmax_weights时，即使regularization_rate设置很小0.001，训练结果仍不理想。\n",
    "    - 猜想：应该是embeddings维度过高，导致惩罚过大\n",
    "- 若只惩罚softmax_weights，当regularization_rate=0.001时，效果与未正则化类似。\n",
    "    - 猜想：训练数据并未过拟合，由于batch_size*num_steps=8*2001=16008，只为训练集规模的两倍左右\n",
    "- 只惩罚softmax_weights，当regularization_rate=0.001，batch_size=64，其余不变，效果提升（0.897到0.915)\n",
    "    - 猜想：效果提升可能只与提高batch_size有关，或者也与正则化有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "# num_sampled = 4 # Number of negative examples to sample.\n",
    "max_sentence_len = 200\n",
    "vocabulary_size = 100000\n",
    "label_size = 4\n",
    "regularization_rate = 0.001\n",
    "graph = tf.Graph()\n",
    "    \n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.int32, shape=[batch_size, max_sentence_len])\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=[batch_size,label_size])\n",
    "  tf_valid_dataset = tf.constant(valid_X, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([label_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([label_size]))\n",
    "  \n",
    "  def model(data): \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    # embedding_lookup()用法: http://blog.csdn.net/u013041398/article/details/60955847\n",
    "    embed = tf.nn.embedding_lookup(embeddings, data)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    sentence_embed = tf.reduce_mean(embed, axis=1)\n",
    "    \n",
    "    return tf.matmul(sentence_embed, tf.transpose(softmax_weights)) + softmax_biases\n",
    "\n",
    "  logits = model(tf_train_dataset)\n",
    "  l2_loss = tf.nn.l2_loss(softmax_weights)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "    + regularization_rate * l2_loss\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  learning_rate = tf.train.exponential_decay(0.01, global_step, 1000, 0.9, staircase=True)\n",
    "  optimizer = tf.contrib.layers.optimize_loss(loss, global_step=global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#   test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.47381 train accuracy: 23.4375\n",
      "loss: 1.11784 train accuracy: 59.375\n",
      "loss: 0.680266 train accuracy: 78.125\n",
      "loss: 0.588923 train accuracy: 84.375\n",
      "loss: 0.417439 train accuracy: 85.9375\n",
      "loss: 0.47924 train accuracy: 89.0625\n",
      "loss: 0.227059 train accuracy: 98.4375\n",
      "loss: 0.455392 train accuracy: 82.8125\n",
      "loss: 0.37587 train accuracy: 89.0625\n",
      "loss: 0.327902 train accuracy: 90.625\n",
      "loss: 0.286586 train accuracy: 95.3125\n",
      "loss: 0.2923 train accuracy: 92.1875\n",
      "loss: 0.291932 train accuracy: 90.625\n",
      "loss: 0.289714 train accuracy: 93.75\n",
      "loss: 0.216585 train accuracy: 96.875\n",
      "loss: 0.279919 train accuracy: 93.75\n",
      "valid accuracy: 91.5027777778\n",
      "loss: 0.236183 train accuracy: 93.75\n",
      "valid accuracy: 91.7083333333\n",
      "loss: 0.250211 train accuracy: 95.3125\n",
      "valid accuracy: 91.3583333333\n",
      "loss: 0.325249 train accuracy: 87.5\n",
      "valid accuracy: 91.6777777778\n",
      "loss: 0.243435 train accuracy: 95.3125\n",
      "valid accuracy: 91.3388888889\n",
      "loss: 0.242334 train accuracy: 96.875\n",
      "valid accuracy: 91.5277777778\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_y.shape[0] - batch_size)\n",
    "    batch_data = train_X[offset:(offset + batch_size)]\n",
    "    batch_labels = train_y[offset:(offset + batch_size)]\n",
    "\n",
    "    feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "    _, l,tp = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    #print(\"loss:\",l,\"acc:\",a,\"label:\",batch_labels,\"prediction:\",p)\n",
    "    if (step % 100 == 0):\n",
    "        train_acc = accuracy(tp, batch_labels)\n",
    "        print(\"loss:\", l, \"train accuracy:\", train_acc)\n",
    "        if step > num_steps * 0.7:\n",
    "            print( \"valid accuracy:\",accuracy(valid_prediction.eval(), valid_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
